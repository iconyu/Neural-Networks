1. 概述
1） 核心：对代价函数C关于权重W和偏置b的偏导表达式,在改变W和b时,C变化的快慢,网络是如何改变的。
2） 关键要素： 两个假设、一个中间变量、四个公式
3） 变量含义：
W(l/jk)表示第l-1层的第k个神经元到第l层的第j个神经元上的权重，
b(l/j)表示第l层的第j个神经元上的偏置，
a(l/j)表示第l层的第j个神经元上的激活值,w(l)为权重矩阵,b(l)为偏置向量。
a(l/j) = sigmod(求和w(l/jk)*a(l-1/k) + b(l/j))
a(l) = sigmod(w(l)*a(l-1) + b(l))
z(l) = w(l)*a(l-1) + b(l)
a(l) = sigmod(z(l))

2.代价函数
1） 公式： C = 1/2n{求平方和||y(x) - a(L)(x)||}
其中n是样本总数,L是神经网络总层数,y = y(x)是样本对应的输出值,a(L)=a(L)(X)是当输入是x时网络输出的激活向量

3.两个假设
1）总体的代价函数等于全部单个样本的代价总和的平均值，C = 1/n{求和C(x)},其中C(x) = 1/2||y-a(L)||^2
2） 代价函数可以认为是神经网络输出的函数

4.一个中间变量
1） 定义：第l层上第j个神经元上的误差=偏导C/偏导Z(l/j)
2）推导： 如果在第L层的第j个神经元的输入z(l/j)上增加一个微小的变化Δz(l/j)，根据误差传递法则，这个微小的变化最终会导致代价C产生{偏导C/偏导Z(l/j)}*Δz(l/j)
的改变,但是只取偏导数部分作为误差.因为偏导部分其实是代价函数C对Z(l/j)这个变量的梯度.反映了C对于z(l/j)变化的敏感程度，如果敏感程度高即使是自变量非常微小
的变化也会引起整个函数值很大的变化，反之如果非常不敏感，即使是自变量很大的变化也不会另函数整体发生多少改变。所以为了简化运算,误差函数只取偏导部分。

5.四个公式
1.输出层误差：{偏导C/偏导Z(l/j)}*sigmod(z(l/j))
1）其中{偏导C/偏导Z(l/j)}表示代价函数C在第l层的第j个神经元随输出的激活值的变化速度。如果C不太依赖这个神经元，那么{偏导C/偏导Z(l/j)}的值理论上接近0，
整个式子为0，也就说明这个神经元基本上没有误差。sigmod(z(l/j))体现了激活函数在z(l/j)这一点上的变化速度。

2.相邻两层之间误差： 误差(L) = ((w(l+1)^T*误差(L+1))⨀sigmod′(Z(L))
假设数据是从输出层往输入层方向流动，可以利用原有矩阵的转置计算，反向传播。

3.偏置偏导： {偏导C/偏导b(l/j)} = 误差(L/j) 
某个神经元偏置的偏导是这个神经元上的误差

4.权重偏导； {偏导C/偏导w(l/jk)} = a(l-1/k)*误差(L/j)
代价函数对于某一条弧上的权重的偏导数等于这条弧左端的神经元的输出与弧右端神经元的误差的乘积。

6.反向传播的算法
1） 输入x： 为输入层设置激活值a(l)
2） 前向传播： 对于l = 2,…,L-1,L层，计算z(l)=w(l)*a(l-1) + b(l)和a(l) = sigmod(z(l))
3） 计算输出层的误差
4） 反向传播误差
5） 计算代价函数的梯度值----权重和偏导

7.反向传播和梯度下降结合
1)输入训练样本集合
2)对每个训练样本：设置对应的输入激活值，并执行：
   前向传播、输出误差、反向传播误差
3)梯度下降
